{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example of Langchain Integration with an Ollama model (LLama2:7b quantified) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import Ollama\n",
    "\n",
    "# Load the model\n",
    "ollama = Ollama(base_url=\"http://localhost:11434\", model=\"llama2:7b-chat-q4_0\")\n",
    "\n",
    "# print(ollama.invoke(\"who is the president of the united states?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case : RAG from wikipedia page to retrieve contextual info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n",
      "/tmp/ipykernel_61520/2349481596.py:16: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the langchain-huggingface package and should be used instead. To use it run `pip install -U langchain-huggingface` and import as `from langchain_huggingface import HuggingFaceEmbeddings`.\n",
      "  embedding = HuggingFaceEmbeddings(\n",
      "/tmp/ipykernel_61520/2349481596.py:34: LangChainDeprecationWarning: The method `Chain.__call__` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use invoke instead.\n",
      "  print(qacahin({'query': query}))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'query': 'Résume le contenu de la page en moins de 500 mots.', 'result': 'The page is a newsletter for an organization focused on artificial intelligence and virtual reality. The newsletter includes several articles and announcements, including:\\n\\n1. A headline about a company called \"Chasseurs de bug\" that received $12 million from Google in 2024.\\n2. An article about a fusion blanket that could potentially produce energy like the sun on Earth if a test is successful.\\n3. A news item about Elon Musk accusing Ukraine of causing a software problem, with Mars as the suspected location of the problem.\\n4. A call to action to subscribe to the newsletter and receive updates on artificial intelligence and virtual reality trends.\\n\\nOverall, the page is focused on promoting the organization\\'s interest in artificial intelligence and virtual reality, and providing news and updates on these topics.'}\n"
     ]
    }
   ],
   "source": [
    "from langchain.document_loaders import WebBaseLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "# Load the wikipedia page\n",
    "loader = WebBaseLoader(\"https://www.lebigdata.fr/comprendre-les-modeles-de-vision-et-de-langage-un-regard-sur-le-vlm\")\n",
    "docs = loader.load()\n",
    "\n",
    "# Split the page into chunks    \n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=0)\n",
    "all_splits = text_splitter.split_documents(docs)\n",
    "\n",
    "# Embed the chunks\n",
    "embedding = HuggingFaceEmbeddings(\n",
    "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "    model_kwargs={\"device\": \"cpu\"}\n",
    ")\n",
    "# Create a vector store\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=all_splits,\n",
    "    embedding=embedding\n",
    ")\n",
    "\n",
    "# Create a retrieval chain\n",
    "qacahin = RetrievalQA.from_chain_type(\n",
    "    llm=ollama, retriever=vectorstore.as_retriever()\n",
    ")\n",
    "\n",
    "# Query the chain\n",
    "query = \"Résume le contenu de la page en moins de 500 mots.\"\n",
    "\n",
    "print(qacahin({'query': query}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Temps d'inférence = 5min (très long)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
