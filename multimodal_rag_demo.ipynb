{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Démonstration du système RAG Multimodal\n",
                "\n",
                "Ce notebook montre comment utiliser le système de Retrieval Augmented Generation (RAG) multimodal qui permet de combiner des requêtes sur des textes et des images grâce aux embeddings CLIP."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Installation des dépendances\n",
                "\n",
                "Assurez-vous d'avoir installé toutes les dépendances nécessaires :"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "!pip install langchain langchain-core langchain-community langchain-ollama\n",
                "!pip install torch transformers clip pymupdf pillow chromadb\n",
                "!pip install matplotlib tqdm numpy requests"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Importer les classes nécessaires"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "import time\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "from PIL import Image\n",
                "import requests\n",
                "from io import BytesIO\n",
                "\n",
                "# Importer notre système RAG multimodal\n",
                "from multimodal_rag import MultimodalEmbedder, MultimodalVectorStore, MultimodalRAG"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Vérifier que Ollama est en cours d'exécution\n",
                "\n",
                "Avant de commencer, assurez-vous que Ollama est en cours d'exécution et que les modèles nécessaires sont installés."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import subprocess\n",
                "\n",
                "# Vérifier si Ollama est en cours d'exécution\n",
                "try:\n",
                "    response = requests.get(\"http://localhost:11434/api/tags\")\n",
                "    if response.status_code == 200:\n",
                "        print(\"✅ Ollama est en cours d'exécution\")\n",
                "        # Afficher les modèles disponibles\n",
                "        models = response.json()[\"models\"]\n",
                "        print(f\"Modèles disponibles: {', '.join([m['name'] for m in models])}\")\n",
                "    else:\n",
                "        print(\"❌ Ollama est en cours d'exécution mais a retourné une erreur\")\n",
                "except Exception as e:\n",
                "    print(f\"❌ Ollama n'est pas en cours d'exécution ou n'est pas accessible: {e}\")\n",
                "    print(\"Lancez Ollama avec la commande: ollama serve\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Initialiser le système RAG multimodal\n",
                "\n",
                "Maintenant, initialisons notre système RAG multimodal :"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Modèle Ollama à utiliser\n",
                "LLM_MODEL = \"qwen2.5:3b\"\n",
                "\n",
                "# Initialiser le système RAG\n",
                "print(\"Initialisation du système RAG multimodal...\")\n",
                "rag_system = MultimodalRAG(llm_name=LLM_MODEL)\n",
                "print(\"Système RAG multimodal initialisé avec succès!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Explorer les embeddings CLIP\n",
                "\n",
                "CLIP (Contrastive Language-Image Pre-training) est un modèle qui aligne les représentations de texte et d'image dans un même espace vectoriel. Voyons comment cela fonctionne :"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.12.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}